{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registry Pattern\n",
    "\n",
    "The Registry Pattern is a design pattern that provides a centralized place to store and retrieve objects or components, often by associating them with unique keys. This pattern is commonly used in systems where dynamic loading or managing extensible components is required.\n",
    "\n",
    "## Use Case 1:\n",
    "\n",
    "In an LLM system, you may have multiple text preprocessors, tokenizers, or reponse generators. The **Registry Pattern** allows you to register and retrieve these components dynamically, making the system modular and easily extensible.\n",
    "\n",
    "#### Python Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComponentRegistry:\n",
    "    def __init__(self):\n",
    "        self._components = {}\n",
    "\n",
    "    def register(self, name, component):\n",
    "        if name in self._components:\n",
    "            raise ValueError(f\"Component '{name}' is already registered.\")\n",
    "        self._components[name] = component\n",
    "\n",
    "    def get(self, name):\n",
    "        if name not in self._components:\n",
    "            raise KeyError(f\"Component '{name}' not found.\")\n",
    "        return self._components[name]\n",
    "    \n",
    "    def list_components(self):\n",
    "        return list(self._components.keys())\n",
    "    \n",
    "class DefaultPreprocessor:\n",
    "    def process(self, text):\n",
    "        return text.lower()\n",
    "    \n",
    "class FancyProcessor:\n",
    "    def process(self, text):\n",
    "        return text.upper()\n",
    "    \n",
    "registry = ComponentRegistry()\n",
    "\n",
    "registry.register(\"default\", DefaultPreprocessor())\n",
    "registry.register(\"fancy\", FancyProcessor())\n",
    "\n",
    "default_processor = registry.get(\"default\")\n",
    "fancy_processor = registry.get(\"fancy\")\n",
    "\n",
    "text = \"Hello, world!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Processor: hello, world!\n",
      "Fancy processor: HELLO, WORLD!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Default Processor: {default_processor.process(text)}\")\n",
    "print(f\"Fancy processor: {fancy_processor.process(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 2:\n",
    "\n",
    "In an LLM framework, you may need different tokenizers for different use cases, such as tokenizing for specific languages or domains. Using the **Registry Pattern** you can register and retrieve tokenizers dynamically.\n",
    "\n",
    "#### Python Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTokenizer:\n",
    "    def tokenize(self, text):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class WhiteSpaceTokenizer(BaseTokenizer):\n",
    "    def tokenize(self, text):\n",
    "        return text.split()\n",
    "    \n",
    "class CharacterTokenizer(BaseTokenizer):\n",
    "    def tokenize(self, text):\n",
    "        return list(text)\n",
    "    \n",
    "registry.register(\"Whitespace\", WhiteSpaceTokenizer())\n",
    "registry.register(\"Character\", CharacterTokenizer())\n",
    "\n",
    "whitespace_tokenizer = registry.get(\"Whitespace\")\n",
    "character_tokenizer = registry.get(\"Character\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitepace Tokenizer: ['Hello,', 'world!']\n",
      "Character Tokenizer: ['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Whitepace Tokenizer: {whitespace_tokenizer.tokenize(text)}\")\n",
    "print(f\"Character Tokenizer: {character_tokenizer.tokenize(text)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patterns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
