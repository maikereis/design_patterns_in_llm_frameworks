{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype Pattern\n",
    "\n",
    "The Prototype Pattern is used to create new objects by copying an existing object, know as the prototype. Is is useful when the cost of creating a new object from scratch is expensive, and you want to clone an existing object with minimal changes.\n",
    "\n",
    "## Use Case 1:\n",
    "\n",
    "In an LLM framework, you might want to clone a base prompt or configuration, adjusting only certain aspects (like the prompt text or the configuration parameters) rather than creating everything from scratch.\n",
    "\n",
    "#### Python Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class Prototype:\n",
    "    def clone(self):\n",
    "        return copy.deepcopy(self)\n",
    "    \n",
    "class LLMRequest(Prototype):\n",
    "    def __init__(self, prompt, model='GPT-3'):\n",
    "        self.prompt = prompt\n",
    "        self.model = model\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"LLMRequest(prompt={self.prompt}, model={self.model})\"\n",
    "    \n",
    "llm_request1 = LLMRequest(\"Tell me a joke.\")\n",
    "\n",
    "\n",
    "llm_request2 = llm_request1.clone()\n",
    "llm_request2.prompt = \"What's the capital of France?\"\n",
    "llm_request3 = llm_request1.clone()\n",
    "llm_request3.model = 'Anthropic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: LLMRequest(prompt=Tell me a joke., model=GPT-3)\n",
      "Cloned: LLMRequest(prompt=What's the capital of France?, model=GPT-3)\n",
      "Cloned with a different model: LLMRequest(prompt=Tell me a joke., model=Anthropic)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\", llm_request1)\n",
    "print(\"Cloned:\", llm_request2)\n",
    "print(\"Cloned with a different model:\", llm_request3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 2:\n",
    "\n",
    "You could use the Prototype Pattern to create similar LLM requests for multiple models, adjusting only the model type or prompt without creating new objects from scratch.\n",
    "\n",
    "#### Python Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModelLLMRequest(LLMRequest):\n",
    "    def __init__(self, prompt, models=['GPT-4', 'Anthropic']):\n",
    "        super().__init__(prompt)\n",
    "        self.models = models\n",
    "\n",
    "    def generate_requests(self):\n",
    "        return [LLMRequest(self.prompt, model) for model in self.models]\n",
    "    \n",
    "multi_model_request = MultiModelLLMRequest(\"Summarize the article.\", [\"GPT-4\", \"Anthropic\", \"Gemini\"])\n",
    "requests = multi_model_request.generate_requests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMRequest(prompt=Summarize the article., model=GPT-4)\n",
      "LLMRequest(prompt=Summarize the article., model=Anthropic)\n",
      "LLMRequest(prompt=Summarize the article., model=Gemini)\n"
     ]
    }
   ],
   "source": [
    "for req in requests:\n",
    "    print(req)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References:\n",
    "\n",
    "1. https://refactoring.guru/design-patterns/prototype\n",
    "\n",
    "2. https://refactoring.guru/design-patterns/prototype/python/example#example-0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patterns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
